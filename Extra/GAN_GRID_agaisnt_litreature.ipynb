{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "yoU94B49NimG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU3ChX6DMh5d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.utils.data as data_utils\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Path of the model (saved/to save)\n",
        "modelFolder = './models/'\n",
        "\n",
        "# When True, retrain the whole model\n",
        "retrain = True\n",
        "\n",
        "# Downsample the dataset\n",
        "ds = True\n",
        "\n",
        "# Size of the split\n",
        "trainSize = 0.75\n",
        "valSize = 0.05\n",
        "testSize = 0.20\n",
        "\n",
        "# Specify number of seconds for the window. Default: 16\n",
        "window_size = 16\n",
        "\n",
        "# Model hyper-parameters\n",
        "batch_size = 4\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "# Classes to drop in the dataset\n",
        "classes_to_drop=[\n",
        "    'stabf','stab']\n",
        "\n"
      ],
      "metadata": {
        "id": "5uHnLGRUgLDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "def setSeed(seed=seed):\n",
        "    \"\"\"\n",
        "    Setting the seed for reproducibility\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "setSeed()\n",
        "\n",
        "def min_max_norm(self,col):\n",
        "    self._data[col]=(self._data[col]-self._data[col].min())/(self._data[col].max()-self._data[col].min())\n",
        "\n",
        "\n",
        "def std_scaler(self,col):\n",
        "    self._data[col]=(self._data[col]-self._data[col].mean())/(self._data[col].std())\n",
        "\n",
        "\n",
        "def f1(test_loader, model):\n",
        "    f1 = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, labels) in enumerate(test_loader):\n",
        "            outputs = model(data)\n",
        "            pred = outputs.data.max(1, keepdim=True)[1]\n",
        "            f1 += f1_score(labels, pred, average='macro')\n",
        "    avg_f1 = f1/len(test_loader)\n",
        "    return (avg_f1)\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_path='/content/new_dataset.csv', classes_to_drop=classes_to_drop, window_size=window_size, normalize=True, normalize_method='mean_std', auth=False, target=None):\n",
        "\n",
        "        self._window_size=window_size\n",
        "        self._data=pd.read_csv(file_path)\n",
        "\n",
        "        # if auth==True:\n",
        "        #     if target != 'J':\n",
        "        #         self._data = self._data[self._data['stabf'].isin([target, 'J'])]\n",
        "        #     else:\n",
        "        #         self._data = self._data[self._data['stabf'].isin([target, 'I'])]\n",
        "\n",
        "        #     self._data['stabf'] = self._data['stabf'].apply(lambda x: target if x == target else 'Z')\n",
        "        #     self._data['stabf'] = self._data['stabf'].map({target: 1, 'Z': 0}).fillna(0).astype(int)\n",
        "\n",
        "\n",
        "        # # Random Undersampling\n",
        "        # X = self._data.drop('stabf', axis=1)\n",
        "        # y = self._data['stabf']\n",
        "\n",
        "        # # sampler = RandomUnderSampler(sampling_strategy='not minority', random_state=seed)\n",
        "        # # X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
        "\n",
        "        # # X_resampled['Class'] = y_resampled\n",
        "        # self._data = X\n",
        "\n",
        "        # The data is sorted by Class A,B,C the indexes of the dataframe have restarted by ignore index\n",
        "        self._data = self._data.sort_values(by=['stabf'], inplace=False,ignore_index = True)\n",
        "\n",
        "        # class_uniq contains the letters of the drivers A,B and it loops across all of them\n",
        "        for class_uniq in list(self._data['stabf'].unique()):\n",
        "            # Find the total number of elements belonging to a class\n",
        "            tot_number=sum(self._data['stabf']==class_uniq)\n",
        "            # Number of elements to drop so that the class element is divisible by window size\n",
        "            to_drop=tot_number%window_size\n",
        "            # Returns the index of the first element of the class\n",
        "            index_to_start_removing=self._data[self._data['stabf']==class_uniq].index[0]\n",
        "            # Drop element from first element to the element required\n",
        "            self._data.drop(self._data.index[index_to_start_removing:index_to_start_removing+to_drop],inplace=True)\n",
        "\n",
        "\n",
        "        # Resetting index of dataframe after dropping values\n",
        "        self._data = self._data.reset_index()\n",
        "        self._data = self._data.drop(['index'], axis=1)\n",
        "\n",
        "        index_starting_class=[] # This array contains the starting index of each class in the df\n",
        "        for class_uniq in list(self._data['stabf'].unique()):\n",
        "            # Appending the index of first element of each clas\n",
        "            index_starting_class.append(self._data[self._data['stabf']==class_uniq].index[0])\n",
        "\n",
        "        # Create the sequence of indexs of the windows\n",
        "        sequences=[]\n",
        "        for i in range(len(index_starting_class)):\n",
        "            # Check if beginning of next class is there\n",
        "            if i!=len(index_starting_class)-1:\n",
        "                ranges=np.arange(index_starting_class[i], index_starting_class[i+1])\n",
        "            else:\n",
        "                ranges = np.arange(index_starting_class[i], len(self._data))\n",
        "            for j in range(0,len(ranges),int(self._window_size/2)):\n",
        "                if len(ranges[j:j+self._window_size])==16:\n",
        "                    sequences.append(ranges[j:j+self._window_size])\n",
        "        self._sequences=sequences\n",
        "\n",
        "\n",
        "        # Take only the 'Class' which are the actual labels and store it in the labels of self\n",
        "        self._labels=self._data['stabf']\n",
        "        # Dropping columns which have constant measurements because they would return nan in std\n",
        "        self._data.drop(classes_to_drop, inplace=True, axis=1)\n",
        "\n",
        "        # Function to normalize the data either with min_max or mean_std\n",
        "        if normalize and not auth:\n",
        "            for col in self._data.columns:\n",
        "                if normalize_method=='min_max':\n",
        "                    min_max_norm(self,col)\n",
        "                elif normalize_method==\"mean_std\":\n",
        "                    std_scaler(self,col)\n",
        "\n",
        "        # Create the array holding the windowed multidimensional arrays\n",
        "        X=np.empty((len(sequences), self._window_size, len(self._data.columns)))\n",
        "        y=[]\n",
        "\n",
        "        for n_row, sequence in enumerate(sequences):\n",
        "            X[n_row,:,:]=self._data.iloc[sequence]\n",
        "            # The corresponding driver of the sequence is the driver at first sequence\n",
        "            y.append(self._labels[sequence[0]])\n",
        "\n",
        "        assert len(y)==len(X)\n",
        "        # Assign the windowed dataset to the X of self\n",
        "        self._X= X\n",
        "\n",
        "        # Targets is a transformed version of y with drivers are encoded into 0 to 9\n",
        "        targets = preprocessing.LabelEncoder().fit_transform(y)\n",
        "        class_labels = encoder.classes_\n",
        "        for code, label in enumerate(class_labels):\n",
        "          print(f'Code: {code} -> Label: {label}')\n",
        "        targets = torch.as_tensor(targets)  # Just converting it to a pytorch tensor\n",
        "        self._y=targets # Assign it to y of self\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._X)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.FloatTensor(self._X[index,:,:]), self._y[index]\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        inputs = inputs\n",
        "        labels = labels\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        # Collect predictions and true labels\n",
        "        y_true += labels.data.cpu().numpy().tolist()\n",
        "        y_pred += preds.cpu().numpy().tolist()\n",
        "\n",
        "    # Calculate accuracy and loss\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "    epoch_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1\n",
        "\n",
        "\n",
        "def evaluateBinary(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            # loss = criterion(outputs, labels)\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        # preds = (outputs > 0.5).float()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        # Collect predictions and true labels\n",
        "        y_true += labels.data.cpu().numpy().tolist()\n",
        "        y_pred += preds.cpu().numpy().tolist()\n",
        "\n",
        "    # Calculate accuracy and loss\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "    epoch_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1\n",
        "\n"
      ],
      "metadata": {
        "id": "EjXMMgcXgo4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/smart_grid_stability_augmented.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "df"
      ],
      "metadata": {
        "id": "WCTrkBiQnHgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "df['stabf'] = encoder.fit_transform(df['stabf'])\n",
        "\n",
        "# Retrieve the mapping of numerical codes to original class labels\n",
        "class_labels = encoder.classes_\n",
        "\n",
        "# Display the mapping\n",
        "for code, label in enumerate(class_labels):\n",
        "    print(f'Code: {code} -> Label: {label}')\n",
        "df"
      ],
      "metadata": {
        "id": "rw0YcgjAPj9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('new_dataset.csv', index=False)\n",
        "df"
      ],
      "metadata": {
        "id": "OkuqT40NhHQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = True\n"
      ],
      "metadata": {
        "id": "o723dQMYnDGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = CustomDataset()\n",
        "\n",
        "# Defining sizes\n",
        "train_size = int(trainSize * len(a))\n",
        "val_size = int(valSize * len(a))\n",
        "test_size = len(a)-train_size-val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    a, [train_size, val_size, test_size])\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=False,\n",
        "                                           drop_last=True)\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                                batch_size=batch_size,\n",
        "                                                shuffle=False,\n",
        "                                                drop_last=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          drop_last=True)"
      ],
      "metadata": {
        "id": "yEMcnGxzg2Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reproduced model from litreature"
      ],
      "metadata": {
        "id": "xYmxCn07ZbCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class MLP_reproduced(torch.nn.Module):\n",
        "    def __init__(self, batch_size, window_size, num_features, dropout_rate=0.25):\n",
        "        super(MLP_reproduced, self).__init__()\n",
        "        self.nn1 = torch.nn.Linear(num_features, 288)\n",
        "        self.relu1 = torch.nn.ReLU()\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
        "        self.nn2 = torch.nn.Linear(288, 24)\n",
        "        self.relu2 = torch.nn.ReLU()\n",
        "        self.nn3 = torch.nn.Linear(24, 12)\n",
        "        self.relu3 = torch.nn.ReLU()\n",
        "        self.fc = torch.nn.Linear(12, 1)  # Output size is 1 for binary classification\n",
        "        self.sigmoid = torch.nn.Sigmoid()  # Sigmoid activation for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        nn1_out = self.relu1(self.nn1(x))\n",
        "        nn1_out = self.dropout(nn1_out)\n",
        "        nn2_out = self.relu2(self.nn2(nn1_out))\n",
        "        nn3_out = self.relu3(self.nn3(nn2_out))\n",
        "        fc_out = self.fc(nn3_out)\n",
        "        out = self.sigmoid(fc_out)\n",
        "        return out[:, -1, :]\n",
        "\n",
        "model = MLP_reproduced(batch_size, window_size, 12).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "retrain = False\n",
        "\n",
        "if not os.path.exists('./models/rnn_auth.pt') or retrain:\n",
        "    # Training loop\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        average_loss = total_loss / len(train_loader)\n",
        "        print(f'[💪 EPOCH {epoch+1}/50] Loss: {average_loss:.3f}')\n"
      ],
      "metadata": {
        "id": "sk0vScXAKCar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    outputs = model(inputs)\n",
        "    predictions = (outputs > 0.5).float()\n",
        "\n",
        "    for p, l in zip(predictions, labels.float()):\n",
        "        if p == l:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    total_samples += labels.size(0)\n",
        "\n",
        "    y_true.extend(labels.cpu().numpy())\n",
        "    y_pred.extend(predictions.cpu().numpy())\n",
        "\n",
        "acc = correct_predictions/total_samples\n",
        "f1 = f1_score(y_true, y_pred, average='binary')\n",
        "\n",
        "print('[👑 TEST GRU AUTH]\\n')\n",
        "print(f'[🎯 ACCURACY] {acc:.3f}')\n",
        "print(f'[⚖️ F1 SCORE] {f1:.3f}')"
      ],
      "metadata": {
        "id": "53BSew72Ocdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN-GRID Attack"
      ],
      "metadata": {
        "id": "oBGmf4GYZlyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, batch_size, window_size, num_features,):\n",
        "        super(Generator, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.num_features = num_features\n",
        "        self.window_size = window_size\n",
        "        self.layer1 = nn.Linear(num_features, 128)\n",
        "        self.layer2 = nn.Linear(128, 256)\n",
        "        self.layer3 = nn.Linear(256, 512)\n",
        "        self.layer4 = nn.Linear(512, batch_size*window_size)\n",
        "        self.layer5 = nn.Linear(batch_size*window_size, num_features)\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.leaky_relu(self.layer1(x))\n",
        "        x = self.leaky_relu(self.layer2(x))\n",
        "        x = self.leaky_relu(self.layer3(x))\n",
        "        x = self.leaky_relu(self.layer4(x))\n",
        "        x = self.layer5(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Qv-QwhKMv4Wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, surrogate, label, train_loader, num_epochs=100, lr=0.001, device=torch.device('cpu'), ml=False, num_episodes=150):\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    if not ml:\n",
        "        generator = generator.to(device)\n",
        "        surrogate = surrogate.to(device)\n",
        "\n",
        "        # for model in surrogate.models:\n",
        "        #     model.to(device)\n",
        "        #     model.train()\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    binary_cross_entropy_loss = nn.BCEWithLogitsLoss()\n",
        "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
        "\n",
        "    # Define the reinforcement learning parameters\n",
        "    max_episode_length = 10\n",
        "    alpha = 0.1\n",
        "    gamma = 0.9\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        # Initialize the latent input and the episode reward\n",
        "        latent_input = torch.randn(4, 16, 12).to(device)\n",
        "        episode_reward = 0\n",
        "\n",
        "        for step in range(max_episode_length):\n",
        "            # Generate a sample with the current latent input\n",
        "            fake_input = generator(latent_input)\n",
        "\n",
        "            # Evaluate the sample with the surrogate model\n",
        "            if not ml:\n",
        "                surrogate_output = surrogate(fake_input)\n",
        "            else:\n",
        "                surrogate_output = []\n",
        "                # Looping through each\n",
        "                for group in fake_input:\n",
        "                    # Flatten the group to make it\n",
        "                    flat_group = group.view(-1, group.size(-1)).detach().numpy()\n",
        "                    # Get the probabilities\n",
        "                    probabilities = surrogate.predict_proba(flat_group)\n",
        "                    mean_probabilities = np.mean(probabilities, axis=0)\n",
        "                    # Append the probabilities to the array\n",
        "                    surrogate_output.append(mean_probabilities)\n",
        "                with torch.no_grad():\n",
        "                    surrogate_output = torch.tensor(surrogate_output, requires_grad=True)\n",
        "\n",
        "            predictions = (surrogate_output > 0.5).float()\n",
        "            targets = torch.randint_like(predictions, 0, 2)\n",
        "            reward = (predictions == targets).float().mean().item()\n",
        "            episode_reward += reward\n",
        "\n",
        "            # Update the latent input using reinforcement learning\n",
        "            td_error = reward - episode_reward\n",
        "            latent_input += alpha * td_error * gamma**step * torch.randn_like(latent_input)\n",
        "\n",
        "        # Update the generator using the final latent input of the episode\n",
        "        generator_optimizer.zero_grad()\n",
        "        fake_input = generator(latent_input)\n",
        "\n",
        "        if not ml:\n",
        "            surrogate_output = surrogate(fake_input)\n",
        "        else:\n",
        "            surrogate_output = []\n",
        "            # Looping through\n",
        "            for group in fake_input:\n",
        "                flat_group = group.view(-1, group.size(-1)).detach().numpy()\n",
        "                # Get the probabilities\n",
        "                probabilities = surrogate.predict_proba(flat_group)\n",
        "                mean_probabilities = np.mean(probabilities, axis=0)\n",
        "                # Append the probabilities to the array\n",
        "                surrogate_output.append(mean_probabilities)\n",
        "            with torch.no_grad():\n",
        "                surrogate_output = torch.tensor(surrogate_output, requires_grad=True)\n",
        "\n",
        "        target_labels = targets.view(-1, 1).float()\n",
        "        target_labels = torch.full_like(target_labels, label)\n",
        "\n",
        "        generator_loss = binary_cross_entropy_loss(surrogate_output, target_labels)\n",
        "\n",
        "        if ml:\n",
        "            generator_optimizer.zero_grad()\n",
        "\n",
        "        generator_loss.backward()\n",
        "        generator_optimizer.step()\n",
        "\n",
        "        losses.append(generator_loss.item())\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            print(f'[⏭️ EP {episode}/{num_episodes} | D{label}] LOSS: {round(generator_loss.item(), 3)}')\n",
        "\n",
        "    print()\n",
        "\n",
        "    return generator, losses"
      ],
      "metadata": {
        "id": "kdZZtKebVuF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 3e-3\n",
        "\n",
        "generators = []\n",
        "losses = []\n",
        "\n",
        "inputs, classes = next(iter(train_loader))\n",
        "\n",
        "# For each driver\n",
        "for d in range(1):\n",
        "        print(f'[🤖 GENERATORS] Label {d}')\n",
        "\n",
        "        batch_size, window_size, num_features = inputs.shape\n",
        "        generator = Generator(batch_size, window_size, num_features)\n",
        "        surrogate_model = model\n",
        "\n",
        "        generator, loss = train_gan(generator, surrogate_model, train_loader=train_loader, num_epochs=20, lr=lr, label=0, ml=False, num_episodes=100)\n",
        "        print()\n",
        "\n",
        "        generators.append(generator)\n",
        "        losses.append(loss)"
      ],
      "metadata": {
        "id": "dHQbv_E5kjU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "for i in range(1):\n",
        "    predicted_labels = []\n",
        "    generator = generators[i].to(device)\n",
        "\n",
        "    for batch in test_loader:\n",
        "        input_batch, true_labels = batch[0].to(device), batch[1].to(device)\n",
        "        # Generate data\n",
        "        generated_data = generator(torch.randn(4, 16, 12).to(device))\n",
        "        # generated_data = generated_data * ones_tensor\n",
        "\n",
        "        # Add the result to the ones tensor\n",
        "        final_result =  generated_data\n",
        "\n",
        "        # Get the surrogate outputs for each sample in the generated data\n",
        "        surrogate_outputs = model(final_result)\n",
        "\n",
        "        # Apply the threshold for binary classification\n",
        "        predicted_labels_batch = (surrogate_outputs > threshold).float()\n",
        "\n",
        "        # Append the predicted labels to the lists\n",
        "        predicted_labels.extend(predicted_labels_batch.squeeze().tolist())  # Squeeze the tensor\n",
        "\n",
        "    asr = predicted_labels.count(0) / len(predicted_labels)\n",
        "    results.append(asr)\n",
        "    print(f'[👑 DRIVER {i}] ASR: {round(asr, 3)}')"
      ],
      "metadata": {
        "id": "OPayR_t6X0Hh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}